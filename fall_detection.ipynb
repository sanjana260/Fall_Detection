{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa698edf",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc6bb7",
   "metadata": {},
   "source": [
    "### Cropping Video\n",
    "\n",
    "The video I downloaded has the footage on the right half, so I'm cropping the left half out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19ed67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def crop_right_half(input_filename, input_folder=\"data/Raw Videos\", output_folder=\"data/Final Videos\"):\n",
    "    \"\"\"\n",
    "    Crops the right half of the video from the specified input folder\n",
    "    and saves the result to the specified output folder.\n",
    "    \n",
    "    Args:\n",
    "        input_filename (str): The name of the input video file.\n",
    "        input_folder (str): The folder containing raw videos.\n",
    "        output_folder (str): The folder to save cropped videos.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    input_path = os.path.join(input_folder, input_filename)\n",
    "    output_path = os.path.join(output_folder, input_filename)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    full_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    half_width = full_width // 2\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (half_width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        right_half = frame[:, full_width//2:]\n",
    "        out.write(right_half)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Cropped video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0dc0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped video saved to data/Final Videos/adl-30-cam0.mp4\n"
     ]
    }
   ],
   "source": [
    "crop_right_half(\"adl-30-cam0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ae1a50eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sanjanamohan/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/__init__.py'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a73e8",
   "metadata": {},
   "source": [
    "# Fall Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180148cb",
   "metadata": {},
   "source": [
    "### YOLOv8-pose\n",
    "\n",
    "Detecting fall based on drop in center of gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585c13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f42ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9953024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.trackers import BYTETracker\n",
    "# Configure tracker\n",
    "tracker_params = {\n",
    "    \"max_age\": 60,\n",
    "    \"min_hits\": 3,\n",
    "    \"iou_threshold\": 0.5,\n",
    "    \"match_thresh\": 0.9,\n",
    "}\n",
    "model = YOLO(\"yolov8l-pose.pt\").to(mps_device)\n",
    "# model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ba2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fall_detection(input_video_path,min_area = 2000):\n",
    "    \"\"\"\n",
    "    Runs fall detection on a given video and writes the annotated output to the 'Outputs' folder.\n",
    "\n",
    "    Args:\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_name (str): (Optional) Name of the output video file. Defaults to 'fall_detection_output.mp4'.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved annotated video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure Outputs directory exists\n",
    "    os.makedirs(\"Outputs\", exist_ok=True)\n",
    "    input_video_filename = os.path.basename(input_video_path)\n",
    "    output_path = os.path.join(\"Outputs\", input_video_filename)\n",
    "    people_images_folder = os.path.splitext(input_video_filename)[0]\n",
    "    people_output_dir = os.path.join('Outputs', people_images_folder + \"_people\")\n",
    "    if os.path.exists(people_output_dir):\n",
    "        shutil.rmtree(people_output_dir)\n",
    "    os.makedirs(people_output_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 20\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(\n",
    "        output_path,\n",
    "        fourcc,\n",
    "        fps,\n",
    "        (width, height)\n",
    "    )\n",
    "\n",
    "    people = {}\n",
    "    prev_timestamp = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_org = frame.copy()\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        # results = model(frame)\n",
    "        results = model.track(frame, persist=True, tracker='./configs/bytetrack.yaml' , conf=0.35)\n",
    "\n",
    "        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        timestamp_sec = timestamp_ms / 1000.0\n",
    "        dt = timestamp_sec - prev_timestamp\n",
    "        prev_timestamp = timestamp_sec\n",
    "        print(dt)\n",
    "\n",
    "        # Get current frame position (0-indexed)\n",
    "        frame_id = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        print(f\"Frame ID: {frame_id}\")\n",
    "        fall_ongoing = False\n",
    "\n",
    "        for result in results:\n",
    "            if result.keypoints is None:\n",
    "                continue\n",
    "\n",
    "            keypoints = result.keypoints.xy.cpu().numpy()\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            ids = result.boxes.id\n",
    "            if ids is None:\n",
    "                continue\n",
    "            ids = ids.cpu().numpy()\n",
    "            # print(ids)\n",
    "\n",
    "            for i, kps in enumerate(keypoints):\n",
    "                box = boxes[i]\n",
    "                person_id = int(ids[i])\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "                if area < min_area:\n",
    "                    continue\n",
    "\n",
    "                left_shoulder = kps[5]\n",
    "                right_shoulder = kps[6]\n",
    "                left_hip = kps[11]\n",
    "                right_hip = kps[12]\n",
    "                nose = kps[0]\n",
    "                left_eye = kps[1]\n",
    "                right_eye = kps[2]\n",
    "\n",
    "                head_y = np.mean([nose[1], left_eye[1], right_eye[1]])\n",
    "\n",
    "                # Compute torso center\n",
    "                shoulder_center = (left_shoulder + right_shoulder) / 2\n",
    "                hip_center = (left_hip + right_hip) / 2\n",
    "\n",
    "                # Identifying and storing info about the people in the frame\n",
    "                if person_id not in people:\n",
    "                    # INSERT_YOUR_CODE\n",
    "                    frame_filename = os.path.join(people_output_dir, f\"{person_id}_{int(frame_id)}.jpg\")\n",
    "                    person_image = frame_org.copy()\n",
    "                    # Draw bounding box & person\n",
    "                    cv2.rectangle(person_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.circle(person_image, tuple(shoulder_center.astype(int)), 5, (255,0,0), -1)\n",
    "                    cv2.circle(person_image, tuple(hip_center.astype(int)), 5, (0,255,0), -1)\n",
    "                    cv2.putText(\n",
    "                            person_image,\n",
    "                            str(person_id),\n",
    "                            (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.9, (255,255,255), 2\n",
    "                        )\n",
    "                    print(frame_filename)\n",
    "                    saved = cv2.imwrite(frame_filename, person_image)\n",
    "                    print(saved)\n",
    "                    people[person_id] = {\n",
    "                        'falls': [],\n",
    "                        'frames':[],\n",
    "                        'fall_ongoing': False,\n",
    "                        'fall_frame':0,\n",
    "                        'fall_done':0,\n",
    "                        'angle_changed':False,\n",
    "                        'prev_angle':0,\n",
    "                        'prev_head':head_y,\n",
    "                        'fall_angle_data':[],\n",
    "                        'angle_changes':[],\n",
    "                        'image':frame_filename\n",
    "                    }\n",
    "                people[person_id]['frames'].append((frame_id,x1,y1,x2,y2))\n",
    "\n",
    "                # Draw bounding box & person\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.circle(frame, tuple(shoulder_center.astype(int)), 5, (255,0,0), -1)\n",
    "                cv2.circle(frame, tuple(hip_center.astype(int)), 5, (0,255,0), -1)\n",
    "                cv2.putText(\n",
    "                        frame,\n",
    "                        str(person_id),\n",
    "                        (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, (255,255,255), 2\n",
    "                    )\n",
    "\n",
    "\n",
    "                # Compute vertical drop\n",
    "                torso_length = np.linalg.norm(shoulder_center - hip_center)\n",
    "                if torso_length > 0:\n",
    "\n",
    "                    # TORSO ANGLE \n",
    "                    dx = shoulder_center[0] - hip_center[0]\n",
    "                    dy = shoulder_center[1] - hip_center[1]\n",
    "\n",
    "                    angle = abs(np.degrees(np.arctan2(abs(dx), abs(dy))))\n",
    "                    angle_change = abs(angle - people[person_id]['prev_angle'])\n",
    "                    people[person_id]['prev_angle'] = angle\n",
    "                    \n",
    "                    is_horizontal = angle > 40\n",
    "\n",
    "                    # BOX RATIO\n",
    "                    box_ratio = (x2 - x1) / (y2 - y1)\n",
    "\n",
    "                    is_wide = box_ratio > 1.2\n",
    "\n",
    "                    # HEAD LOW\n",
    "                    head_low = head_y > frame_height * 0.7\n",
    "\n",
    "                    head_change = head_y - people[person_id]['prev_head']\n",
    "                    people[person_id]['prev_head'] = head_y\n",
    "\n",
    "                    fall_speed = head_change/dt\n",
    "\n",
    "                    is_fall = is_horizontal and is_wide and head_low\n",
    "                    print(\"Fall speed: \", fall_speed, is_fall)\n",
    "\n",
    "                    angle_changed = angle_change > 10\n",
    "                    people[person_id]['angle_changed'] = angle_changed\n",
    "\n",
    "                    if angle_changed:\n",
    "                        people[person_id]['fall_angle_data'].append({\n",
    "                            'timestamp':timestamp_sec,\n",
    "                            'frame_id':frame_id,\n",
    "                            'angle':angle,\n",
    "                            'angle_change':angle_change,\n",
    "                            'fall_speed':fall_speed,\n",
    "                            'person_id':person_id\n",
    "                            })\n",
    "                        fall_ongoing = True\n",
    "\n",
    "                    people[person_id]['angle_changes'].append((angle, angle_change, is_fall))\n",
    "\n",
    "                    if is_fall:\n",
    "                        people[person_id]['fall_ongoing'] = is_fall\n",
    "                        people[person_id]['fall_frame'] = frame_id\n",
    "                        people[person_id]['fall_done'] = 0\n",
    "                        label = \"FALL [{}°]\".format(angle)\n",
    "                        color = (0,0,255)\n",
    "                    elif angle_changed:\n",
    "                        label = \"TILTING [{}°]\".format(angle)\n",
    "                        color = (0, 165, 255)\n",
    "                    else:\n",
    "                        if people[person_id]['fall_ongoing']:\n",
    "                            fall_done = people[person_id]['fall_done']\n",
    "                            if fall_done>10:\n",
    "                                fall_ongoing = False\n",
    "                                # people[person_id]['fall_ongoing'] = False\n",
    "                                people[person_id]['angle_changed'] = False\n",
    "                                fall_angle_data = people[person_id]['fall_angle_data']\n",
    "                                people[person_id]['falls'].append({\n",
    "                                    'person_id':person_id,\n",
    "                                    'angle_changes':fall_angle_data,\n",
    "                                    'fall_frame':people[person_id]['fall_frame'],\n",
    "                                    'fall_start':(fall_angle_data[0]['frame_id'],fall_angle_data[0]['timestamp']),\n",
    "                                    'fall_end':(fall_angle_data[-1]['frame_id'],fall_angle_data[-1]['timestamp'])\n",
    "                                })\n",
    "                                people[person_id]['fall_angle_data'] = []\n",
    "                                people[person_id]['fall_done'] = 0\n",
    "                            else:\n",
    "                                people[person_id]['fall_done'] +=1\n",
    "                        label = \"No Fall [{}°]\".format(angle)\n",
    "                        color = (0,255,0)\n",
    "\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        label,\n",
    "                        (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, color, 2\n",
    "                    )\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        if fall_ongoing:\n",
    "            # Create a color overlay\n",
    "            overlay = np.zeros_like(frame)\n",
    "            overlay[:] = (0, 0, 255)  # Red tint\n",
    "\n",
    "            # Blend: alpha controls transparency\n",
    "            alpha = 0.3\n",
    "            frame = cv2.addWeighted(frame, 1 - alpha, overlay, alpha, 0)\n",
    "        out.write(frame)\n",
    "\n",
    "    for person_id in people:\n",
    "        if people[person_id]['fall_ongoing']:\n",
    "            fall_angle_data = people[person_id]['fall_angle_data']\n",
    "            people[person_id]['falls'].append({\n",
    "                                    'person_id':person_id,\n",
    "                                    'angle_changes':fall_angle_data,\n",
    "                                    'fall_start':(fall_angle_data[0]['frame_id'],fall_angle_data[0]['timestamp']),\n",
    "                                    'fall_end':(fall_angle_data[-1]['frame_id'],fall_angle_data[-1]['timestamp'])\n",
    "                                })\n",
    "        # Store each person's photo \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Fall detection video saved to {output_path}\")\n",
    "    return output_path,people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path, people = run_fall_detection(\"data/Final Videos/fall-01-cam0-right-half.mp4\")\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9ae18e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Image:  Outputs/fall-01-cam0-right-half_people/3_2.jpg\n",
      "No of falls:  1\n",
      "(111.0, 3.6666666666666665) (140.0, 4.633333333333333)\n",
      "111.0 75.60579 312.06204 3\n",
      "116.0 56.47116 244.55931 3\n",
      "123.0 3.9499733 -1061.011 3\n",
      "125.0 73.1588 585.10114 3\n",
      "132.0 65.095665 -33.251953 3\n",
      "133.0 2.9400218 13.457793 3\n",
      "134.0 58.422512 -41.577755 3\n",
      "139.0 43.860653 79.4284 3\n",
      "140.0 68.83916 -63.57971 3\n"
     ]
    }
   ],
   "source": [
    "for person_id in people:\n",
    "    print(person_id)\n",
    "    person = people[person_id]\n",
    "    falls = person['falls']\n",
    "    print(\"Image: \",person['image'])\n",
    "    print(\"No of falls: \",len(falls))\n",
    "    for fall in falls:\n",
    "        print(fall['fall_start'],fall['fall_end'])\n",
    "        for frame in fall['angle_changes']:\n",
    "            # print(frame['fall_speed'])\n",
    "            print(frame['frame_id'],frame['angle'],frame['fall_speed'],frame['person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "073507cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALL 35\n",
      "3.9 4.0 35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "39\n",
      "35\n",
      "39\n",
      "35\n",
      "39\n",
      "35\n",
      "FALL 35\n",
      "4.2 4.233333333333333 35\n",
      "39\n",
      "35\n",
      "39\n",
      "35\n",
      "39\n",
      "35\n",
      "35\n",
      "FALL 35\n",
      "4.566666666666666 4.566666666666666 35\n",
      "35\n",
      "35\n",
      "35\n",
      "FALL 35\n",
      "4.9 4.9 35\n",
      "39\n",
      "35\n",
      "39\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for fall in falls:\n",
    "    person_id = fall['person_id']\n",
    "    print(f\"FALL {person_id}\")\n",
    "    print(fall['fall_start'], fall['fall_end'],fall['person_id'])\n",
    "    fall_angle_data = fall['angle_changes']\n",
    "    # print(fall_angle_data)\n",
    "    for angle in fall_angle_data:\n",
    "        print(angle['person_id'])\n",
    "        # angle['person_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "636d2989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "(2.0, 190, 48, 233, 162)\n",
      "(160.0, 100, 186, 206, 240)\n",
      "37\n",
      "(47.0, 0, 113, 53, 226)\n",
      "(48.0, 0, 113, 53, 221)\n",
      "39\n",
      "(98.0, 0, 113, 53, 233)\n",
      "(156.0, 0, 114, 53, 232)\n"
     ]
    }
   ],
   "source": [
    "for person in people:\n",
    "    start_frame = people[person][0]\n",
    "    end_frame = people[person][-1]\n",
    "    print(person)\n",
    "    print(start_frame)\n",
    "    print(end_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a592e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d0c427d",
   "metadata": {},
   "source": [
    "## Live Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f22f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy._core.numeric import True_\n",
    "from sympy.logic import true\n",
    "from sympy.ntheory import continued_fraction, continued_fraction_convergents\n",
    "\n",
    "\n",
    "def run_fall_detection_live(input_video_path, output_filename, min_area = 2000):\n",
    "    \"\"\"\n",
    "    Runs fall detection on a given video and writes the annotated output to the 'Outputs' folder.\n",
    "\n",
    "    Args:\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_name (str): (Optional) Name of the output video file. Defaults to 'fall_detection_output.mp4'.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved annotated video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure Outputs directory exists\n",
    "    os.makedirs(\"Outputs\", exist_ok=True)\n",
    "    # input_video_filename = os.path.basename(input_video_path)\n",
    "    output_path = os.path.join(\"Outputs\", output_filename)\n",
    "    people_images_folder = os.path.splitext(output_filename)[0]\n",
    "    people_output_dir = os.path.join('Outputs', people_images_folder + \"/people\")\n",
    "    if os.path.exists(people_output_dir):\n",
    "        shutil.rmtree(people_output_dir)\n",
    "    os.makedirs(people_output_dir)\n",
    "\n",
    "    fall_output_dir = os.path.join('Outputs', people_images_folder + \"/falls\")\n",
    "    if os.path.exists(fall_output_dir):\n",
    "        shutil.rmtree(fall_output_dir)\n",
    "    os.makedirs(fall_output_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path, cv2.CAP_AVFOUNDATION)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 20\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(\n",
    "        output_path,\n",
    "        fourcc,\n",
    "        fps,\n",
    "        (width, height)\n",
    "    )\n",
    "\n",
    "    people = {}\n",
    "    prev_timestamp = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_org = frame.copy()\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        # results = model(frame)\n",
    "        results = model.track(frame, persist=True, tracker='./configs/bytetrack.yaml' , conf=0.35)\n",
    "\n",
    "        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        timestamp_sec = timestamp_ms / 1000.0\n",
    "        dt = timestamp_sec - prev_timestamp\n",
    "        prev_timestamp = timestamp_sec\n",
    "        print(dt)\n",
    "\n",
    "        # Get current frame position (0-indexed)\n",
    "        frame_id = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        print(f\"Frame ID: {frame_id}\")\n",
    "        fall_ongoing = False\n",
    "\n",
    "        for result in results:\n",
    "            if result.keypoints is None:\n",
    "                continue\n",
    "\n",
    "            keypoints = result.keypoints.xy.cpu().numpy()\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            keypoints_conf = result.keypoints.conf.cpu().numpy()\n",
    "            # print(keypoints_conf)\n",
    "            ids = result.boxes.id\n",
    "            if ids is None:\n",
    "                continue\n",
    "            ids = ids.cpu().numpy()\n",
    "            # print(ids)\n",
    "\n",
    "            for i, kps in enumerate(keypoints):\n",
    "                box = boxes[i]\n",
    "                person_id = int(ids[i])\n",
    "                confs = keypoints_conf[i]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "                if area < min_area:\n",
    "                    continue\n",
    "\n",
    "                left_shoulder = kps[5]\n",
    "                right_shoulder = kps[6]\n",
    "\n",
    "                shoulder_conf = (confs[5] + confs[6])/2\n",
    "                if shoulder_conf<0.5:\n",
    "                    continue\n",
    "\n",
    "                left_hip = kps[11]\n",
    "                right_hip = kps[12]\n",
    "                hip_conf = (confs[11] + confs[12])/2\n",
    "                if hip_conf<0.5:\n",
    "                    continue\n",
    "\n",
    "                nose = kps[0]\n",
    "                left_eye = kps[1]\n",
    "                right_eye = kps[2]\n",
    "\n",
    "                head_y = np.mean([nose[1], left_eye[1], right_eye[1]])\n",
    "\n",
    "                # Compute torso center\n",
    "                shoulder_center = (left_shoulder + right_shoulder) / 2\n",
    "                # print(\"Shoulder center confidence: \",shoulder_center[2])\n",
    "                hip_center = (left_hip + right_hip) / 2\n",
    "\n",
    "                # Identifying and storing info about the people in the frame\n",
    "                if person_id not in people:\n",
    "                    # INSERT_YOUR_CODE\n",
    "                    frame_filename = os.path.join(people_output_dir, f\"{person_id}_{int(frame_id)}.jpg\")\n",
    "                    person_image = frame_org.copy()\n",
    "                    # Draw bounding box & person\n",
    "                    cv2.rectangle(person_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.circle(person_image, tuple(shoulder_center.astype(int)), 5, (255,0,0), -1)\n",
    "                    cv2.circle(person_image, tuple(hip_center.astype(int)), 5, (0,255,0), -1)\n",
    "                    cv2.putText(\n",
    "                            person_image,\n",
    "                            str(person_id),\n",
    "                            (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.9, (255,255,255), 2\n",
    "                        )\n",
    "                    print(frame_filename)\n",
    "                    saved = cv2.imwrite(frame_filename, person_image)\n",
    "                    print(saved)\n",
    "                    people[person_id] = {\n",
    "                        'falls': [],\n",
    "                        'frames':[],\n",
    "                        'fall_ongoing': False,\n",
    "                        'fallen':False,\n",
    "                        'fall_frame':0,\n",
    "                        'fall_done':0,\n",
    "                        'angle_changed':False,\n",
    "                        'prev_angle':0,\n",
    "                        'prev_head':head_y,\n",
    "                        'fall_angle_data':[],\n",
    "                        'angle_changes':[],\n",
    "                        'horizontal':False,\n",
    "                        'vertical':False,\n",
    "                        'image':frame_filename\n",
    "                    }\n",
    "\n",
    "                # Draw bounding box & person\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.circle(frame, tuple(shoulder_center.astype(int)), 5, (255,0,0), -1)\n",
    "                cv2.circle(frame, tuple(hip_center.astype(int)), 5, (0,255,0), -1)\n",
    "                cv2.putText(\n",
    "                        frame,\n",
    "                        str(person_id),\n",
    "                        (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, (255,255,255), 2\n",
    "                    )\n",
    "\n",
    "\n",
    "                # Compute vertical drop\n",
    "                torso_length = np.linalg.norm(shoulder_center - hip_center)\n",
    "                if torso_length > 0:\n",
    "\n",
    "                    # TORSO ANGLE \n",
    "                    dx = shoulder_center[0] - hip_center[0]\n",
    "                    dy = shoulder_center[1] - hip_center[1]\n",
    "\n",
    "                    angle = abs(np.degrees(np.arctan2(abs(dx), abs(dy))))\n",
    "                    angle_change = abs(angle - people[person_id]['prev_angle'])\n",
    "                    people[person_id]['prev_angle'] = angle\n",
    "                    \n",
    "                    is_horizontal = angle > 40\n",
    "                    people[person_id]['horizontal']=is_horizontal\n",
    "\n",
    "                    # BOX RATIO\n",
    "                    box_ratio = (x2 - x1) / (y2 - y1)\n",
    "\n",
    "                    is_wide = box_ratio > 1.2\n",
    "\n",
    "                    # HEAD LOW\n",
    "                    head_low = head_y > frame_height * 0.7\n",
    "\n",
    "                    is_vertical = angle < 5 and not head_low\n",
    "                    people[person_id]['vertical'] = is_vertical\n",
    "\n",
    "                    head_change = head_y - people[person_id]['prev_head']\n",
    "                    people[person_id]['prev_head'] = head_y\n",
    "\n",
    "                    angle_changed = angle_change > 10\n",
    "                    # people[person_id]['angle_changed'] = angle_changed\n",
    "\n",
    "                    is_fall = is_horizontal and is_wide and head_low and angle_changed\n",
    "                    print(\"Head change and angle change: \", head_change, angle_change)\n",
    "\n",
    "                    if angle_change > 10 or head_change < -5:\n",
    "                        people[person_id]['fall_ongoing'] = True\n",
    "                    if people[person_id]['fall_ongoing']:\n",
    "                        print(\"falling\")\n",
    "                        people[person_id]['fall_angle_data'].append({\n",
    "                            'timestamp':float(timestamp_sec),\n",
    "                            'frame_id':frame_id,\n",
    "                            'angle':float(angle),\n",
    "                            'angle_change':float(angle_change),\n",
    "                            'head_y':float(head_y),\n",
    "                            'shoulder_y':float(shoulder_center[1]),\n",
    "                            'head_change':float(head_change),\n",
    "                            })\n",
    "                        \n",
    "                    \n",
    "                    if is_vertical:\n",
    "                        if people[person_id]['fall_ongoing']:\n",
    "                            print(\"Back up\")\n",
    "                        if people[person_id]['fallen']:\n",
    "                            print(\"Recovered\")\n",
    "                            people[person_id]['fallen'] = False\n",
    "                        people[person_id]['angle_changed'] = False\n",
    "                        people[person_id]['fall_ongoing'] = False\n",
    "                        people[person_id]['fall_angle_data'] = []\n",
    "\n",
    "                    # people[person_id]['angle_changes'].append((angle, angle_change, head_change, is_fall))\n",
    "                    people[person_id]['frames'].append({\n",
    "                            'timestamp':float(timestamp_sec),\n",
    "                            'frame_id':frame_id,\n",
    "                            'horizontal':is_horizontal,\n",
    "                            'vertical':is_vertical,\n",
    "                            'angle':float(angle),\n",
    "                            'angle_change':float(angle_change),\n",
    "                            'head_y':float(head_y),\n",
    "                            'shoulder_y':float(shoulder_center[1]),\n",
    "                            'head_change':float(head_change),\n",
    "                            'fall_ongoing':people[person_id]['fall_ongoing'],\n",
    "                            })\n",
    "\n",
    "                    if people[person_id]['horizontal'] and people[person_id]['fallen']:\n",
    "                        continue\n",
    "\n",
    "                    if is_fall:\n",
    "                        print(\"Fell\")\n",
    "                        people[person_id]['fallen'] = True\n",
    "                        people[person_id]['fall_frame'] = frame_id\n",
    "                        fall_ongoing = True\n",
    "                        fall_angle_data = people[person_id]['fall_angle_data']\n",
    "                        dy = fall_angle_data[-1]['timestamp'] - fall_angle_data[0]['timestamp']\n",
    "                        if dy == 0:\n",
    "                            dy = 0.0001\n",
    "                        \n",
    "                        accel_head = []\n",
    "                        # INSERT_YOUR_CODE\n",
    "                        # Calculate acceleration of head from fall_angle_data\n",
    "                        # We'll use the difference of head_change between consecutive frames divided by the time difference\n",
    "                        if len(fall_angle_data) > 2:\n",
    "                            for i in range(1, len(fall_angle_data)):\n",
    "                                dt = fall_angle_data[i]['timestamp'] - fall_angle_data[i-1]['timestamp']\n",
    "                                if dt == 0:\n",
    "                                    accel_head.append(0)\n",
    "                                else:\n",
    "                                    delta_v = fall_angle_data[i]['head_change'] - fall_angle_data[i-1]['head_change']\n",
    "                                    accel = delta_v / dt\n",
    "                                    accel_head.append(accel)\n",
    "                        else:\n",
    "                            # Not enough data for meaningful acceleration\n",
    "                            accel_head = []\n",
    "                        accel_head = sum(accel_head)/len(accel_head)\n",
    "                            \n",
    "                        fall_data = {\n",
    "                            'fall_id':len(people[person_id]['falls']),\n",
    "                            'no_of_people':len(ids),\n",
    "                            'person_id':str(person_id),\n",
    "                            'angle_changes':fall_angle_data,\n",
    "                            'fall_frame':people[person_id]['fall_frame'],\n",
    "                            'head_speed':(fall_angle_data[0]['head_y'] - fall_angle_data[-1]['head_y'])/dy,\n",
    "                            'head_acceleration':accel_head,\n",
    "                            'shoulder_speed':(fall_angle_data[0]['shoulder_y'] - fall_angle_data[-1]['shoulder_y'])/dy,\n",
    "                            'fall_start':(fall_angle_data[0]['frame_id'],fall_angle_data[0]['timestamp']),\n",
    "                            'fall_end':(fall_angle_data[-1]['frame_id'],fall_angle_data[-1]['timestamp'])\n",
    "                        }\n",
    "                        people[person_id]['falls'].append(fall_data)\n",
    "                        fall_filename = str(person_id) + \"_\" + str(len(people[person_id]['falls'])) + '.json'\n",
    "                        with open(os.path.join(fall_output_dir, fall_filename), 'w') as file:\n",
    "                            json.dump(json.dumps(fall_data), file, indent=4)\n",
    "                        people[person_id]['fall_angle_data'] = []\n",
    "                        people[person_id]['fall_ongoing'] = False\n",
    "                        label = \"FALL [{}°]\".format(angle)\n",
    "                        color = (0,0,255)\n",
    "                    elif angle_changed:\n",
    "                        label = \"FALLING [{}°]\".format(angle)\n",
    "                        color = (0, 165, 255)\n",
    "                    else:\n",
    "                        label = \"No Fall [{}°]\".format(angle)\n",
    "                        color = (0,255,0)\n",
    "\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        label,\n",
    "                        (x1-5, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, color, 2\n",
    "                    )\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        if fall_ongoing:\n",
    "            # Create a color overlay\n",
    "            overlay = np.zeros_like(frame)\n",
    "            overlay[:] = (0, 0, 255)  # Red tint\n",
    "\n",
    "            # Blend: alpha controls transparency\n",
    "            alpha = 0.3\n",
    "            frame = cv2.addWeighted(frame, 1 - alpha, overlay, alpha, 0)\n",
    "        out.write(frame)\n",
    "        print('here')\n",
    "        cv2.imshow(\"Live Fall Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "           break\n",
    "\n",
    "    # for person_id in people:\n",
    "    #     if people[person_id]['fall_ongoing']:\n",
    "    #         fall_angle_data = people[person_id]['fall_angle_data']\n",
    "    #         people[person_id]['falls'].append({\n",
    "    #                                 'person_id':person_id,\n",
    "    #                                 'angle_changes':fall_angle_data,\n",
    "    #                                 'fall_start':(fall_angle_data[0]['frame_id'],fall_angle_data[0]['timestamp']),\n",
    "    #                                 'fall_end':(fall_angle_data[-1]['frame_id'],fall_angle_data[-1]['timestamp'])\n",
    "    #                             })\n",
    "        # Store each person's photo \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Fall detection video saved to {output_path}\")\n",
    "    return output_path,people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20738fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 186.2ms\n",
      "Speed: 13.1ms preprocess, 186.2ms inference, 33.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 59.9ms\n",
      "Speed: 2.5ms preprocess, 59.9ms inference, 17.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 1.9ms preprocess, 35.8ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 1.7ms preprocess, 35.5ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 1.8ms preprocess, 36.4ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.0ms preprocess, 35.4ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 2.2ms preprocess, 42.0ms inference, 39.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 3.1ms preprocess, 39.2ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 3.2ms preprocess, 36.7ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 1.9ms preprocess, 35.3ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 1.7ms preprocess, 35.0ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 1.9ms preprocess, 34.9ms inference, 20.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.0ms preprocess, 34.6ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 1.8ms preprocess, 34.7ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.2ms preprocess, 34.8ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.4ms preprocess, 34.6ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 2.4ms preprocess, 35.9ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.3ms preprocess, 35.4ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 1.7ms preprocess, 36.3ms inference, 34.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.1ms preprocess, 35.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.2ms preprocess, 35.1ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.7ms preprocess, 34.8ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.4ms preprocess, 34.6ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.2ms preprocess, 35.2ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.6ms preprocess, 34.6ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.2ms preprocess, 34.6ms inference, 19.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.1ms preprocess, 35.0ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.3ms preprocess, 35.4ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 3.0ms preprocess, 34.5ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.2ms preprocess, 35.0ms inference, 19.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.6ms preprocess, 34.7ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 1.6ms preprocess, 47.3ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 2.5ms preprocess, 36.2ms inference, 19.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.2ms preprocess, 34.6ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 2.1ms preprocess, 34.5ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.0ms preprocess, 35.2ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.1ms preprocess, 34.6ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 2.8ms preprocess, 36.7ms inference, 22.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 2.2ms preprocess, 35.5ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 2.1ms preprocess, 36.1ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.7ms preprocess, 34.7ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.0ms preprocess, 35.2ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.2ms preprocess, 35.4ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.2ms preprocess, 34.7ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.2ms preprocess, 35.4ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.2ms preprocess, 34.9ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.2ms preprocess, 34.7ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.3ms preprocess, 34.7ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.6ms preprocess, 35.3ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.0ms preprocess, 34.7ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 1.8ms preprocess, 34.7ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.4ms preprocess, 34.8ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 2.4ms preprocess, 34.5ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.2ms preprocess, 35.2ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.2ms preprocess, 34.9ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.5ms preprocess, 34.9ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.3ms\n",
      "Speed: 2.4ms preprocess, 34.3ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.0ms preprocess, 34.9ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.0ms preprocess, 34.8ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.4ms preprocess, 34.6ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.6ms preprocess, 35.1ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.2ms preprocess, 35.3ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.5ms preprocess, 34.6ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 2.3ms preprocess, 35.7ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.6ms preprocess, 35.3ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 3.1ms preprocess, 34.8ms inference, 18.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.1ms preprocess, 35.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.2ms preprocess, 35.3ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.1ms preprocess, 35.0ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.4ms preprocess, 34.9ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.2ms preprocess, 35.0ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.1ms preprocess, 34.7ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 2.1ms preprocess, 34.5ms inference, 18.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 2.2ms preprocess, 34.4ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.9ms preprocess, 35.1ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.4ms preprocess, 34.8ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.2ms preprocess, 34.7ms inference, 19.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 3.4ms preprocess, 36.3ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 1.6ms preprocess, 34.9ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 1.4ms preprocess, 35.4ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.0ms preprocess, 35.2ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 2.1ms preprocess, 35.8ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 2.1ms preprocess, 35.8ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 1.5ms preprocess, 37.3ms inference, 19.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 2.0ms preprocess, 36.4ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 2.3ms preprocess, 36.3ms inference, 19.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 3.0ms preprocess, 36.2ms inference, 19.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 1.8ms preprocess, 35.8ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.2ms preprocess, 35.4ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 1.9ms preprocess, 34.7ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 1.7ms preprocess, 36.4ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 1.6ms preprocess, 35.6ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 1.8ms preprocess, 36.4ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 1.7ms preprocess, 36.1ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 1.9ms preprocess, 35.3ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 1.9ms preprocess, 35.4ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 2.0ms preprocess, 35.5ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 2.0ms preprocess, 36.4ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 1.8ms preprocess, 36.7ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.6ms preprocess, 36.0ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 2.1ms preprocess, 35.6ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 3.5ms preprocess, 39.8ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 1.7ms preprocess, 34.7ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.0ms preprocess, 34.6ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 1.9ms preprocess, 35.1ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 1.9ms preprocess, 35.5ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.2ms preprocess, 35.3ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.6ms preprocess, 35.0ms inference, 19.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 2.1ms preprocess, 36.6ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 2.7ms preprocess, 36.5ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 1.7ms preprocess, 38.1ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 3.1ms preprocess, 36.8ms inference, 20.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Outputs/test0/people/1_0.jpg\n",
      "True\n",
      "Head change and angle change:  0.0 1.0809096\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 1.9ms preprocess, 35.0ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -10.780121 0.4052465\n",
      "falling\n",
      "Back up\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 2.0ms preprocess, 35.5ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  14.288666 0.45632732\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 2.1ms preprocess, 36.0ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  16.380737 0.5091641\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.2ms preprocess, 35.0ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  10.123871 3.3382223\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.1ms preprocess, 35.2ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  4.3401184 2.5576215\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.3ms preprocess, 35.0ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  3.969635 1.8652563\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.2ms preprocess, 35.3ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  0.4625244 0.79344225\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.2ms preprocess, 35.3ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -2.8875122 0.38693428\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.2ms preprocess, 35.0ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -7.644043 0.015204906\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.3ms preprocess, 35.2ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -10.481506 1.2932725\n",
      "falling\n",
      "Back up\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.6ms preprocess, 35.2ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -4.689453 0.671952\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.4ms preprocess, 35.2ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -2.838745 0.12479329\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.1ms preprocess, 35.4ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  1.2965088 0.56898165\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.1ms preprocess, 35.2ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -4.5152283 0.46782684\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.2ms preprocess, 35.2ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -1.440979 2.0704126\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.2ms preprocess, 34.7ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -1.0171814 2.373973\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 2.0ms preprocess, 35.5ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -3.2740784 0.14723015\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.1ms preprocess, 35.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -3.1304016 0.06357145\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.0ms preprocess, 34.9ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -5.770752 1.3388882\n",
      "falling\n",
      "Back up\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.5ms preprocess, 34.9ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -3.3430786 0.086817265\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.2ms preprocess, 35.4ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -2.7178955 0.55376697\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 2.0ms preprocess, 36.3ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -2.2998962 0.23219943\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.9ms preprocess, 36.0ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -5.9273987 1.3087299\n",
      "falling\n",
      "Back up\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.5ms preprocess, 34.9ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -2.8893738 0.37055588\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 2.0ms preprocess, 36.3ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  4.1286926 3.3296793\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.1ms preprocess, 35.1ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  13.569153 5.7984843\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.1ms preprocess, 34.9ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  27.286377 5.8602705\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.2ms preprocess, 35.0ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.2ms preprocess, 35.2ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 1.9ms preprocess, 34.8ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.0ms preprocess, 35.2ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.2ms preprocess, 34.8ms inference, 37.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 37.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 (no detections), 35.3ms\n",
      "Speed: 2.4ms preprocess, 35.3ms inference, 16.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 (no detections), 34.8ms\n",
      "Speed: 2.2ms preprocess, 34.8ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.2ms preprocess, 34.9ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.1ms preprocess, 35.3ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 2.1ms preprocess, 35.7ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.4ms preprocess, 34.8ms inference, 35.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.0ms preprocess, 34.8ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 2.4ms preprocess, 34.7ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 2.8ms preprocess, 34.5ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.1ms preprocess, 34.8ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.3ms preprocess, 34.8ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.2ms preprocess, 34.8ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 2.5ms preprocess, 35.3ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.0ms preprocess, 35.1ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 1.5ms preprocess, 35.4ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 2.0ms preprocess, 34.6ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 2.8ms preprocess, 35.0ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.2ms preprocess, 35.1ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.1ms preprocess, 35.1ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 2.9ms preprocess, 34.9ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 2.0ms preprocess, 35.1ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 1.8ms preprocess, 34.8ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.2ms preprocess, 34.8ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 2.4ms preprocess, 35.2ms inference, 34.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.3ms\n",
      "Speed: 2.8ms preprocess, 35.3ms inference, 18.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.9ms\n",
      "Speed: 2.0ms preprocess, 35.9ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 38.2ms\n",
      "Speed: 2.2ms preprocess, 38.2ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 36.0ms\n",
      "Speed: 2.1ms preprocess, 36.0ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 34.9ms\n",
      "Speed: 2.1ms preprocess, 34.9ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 34.6ms\n",
      "Speed: 1.8ms preprocess, 34.6ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 34.6ms\n",
      "Speed: 2.2ms preprocess, 34.6ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.4ms\n",
      "Speed: 2.0ms preprocess, 35.4ms inference, 17.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 34.8ms\n",
      "Speed: 2.0ms preprocess, 34.8ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.0ms\n",
      "Speed: 2.9ms preprocess, 35.0ms inference, 20.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.8ms\n",
      "Speed: 2.4ms preprocess, 35.8ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 35.2ms\n",
      "Speed: 2.1ms preprocess, 35.2ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 92.4ms\n",
      "Speed: 2.8ms preprocess, 92.4ms inference, 33.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 65.2ms\n",
      "Speed: 2.2ms preprocess, 65.2ms inference, 44.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 64.1ms\n",
      "Speed: 1.9ms preprocess, 64.1ms inference, 31.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 62.5ms\n",
      "Speed: 2.1ms preprocess, 62.5ms inference, 28.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 61.6ms\n",
      "Speed: 2.1ms preprocess, 61.6ms inference, 31.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 63.9ms\n",
      "Speed: 2.7ms preprocess, 63.9ms inference, 32.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 63.6ms\n",
      "Speed: 2.1ms preprocess, 63.6ms inference, 32.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 62.7ms\n",
      "Speed: 2.0ms preprocess, 62.7ms inference, 32.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 65.1ms\n",
      "Speed: 2.6ms preprocess, 65.1ms inference, 33.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 77.0ms\n",
      "Speed: 2.8ms preprocess, 77.0ms inference, 38.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Outputs/test0/people/3_0.jpg\n",
      "True\n",
      "Head change and angle change:  0.0 3.201276\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 86.1ms\n",
      "Speed: 2.0ms preprocess, 86.1ms inference, 39.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -115.28963 1.3840368\n",
      "falling\n",
      "Back up\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 76.0ms\n",
      "Speed: 2.7ms preprocess, 76.0ms inference, 38.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -10.355957 1.9831796\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 86.5ms\n",
      "Speed: 2.5ms preprocess, 86.5ms inference, 42.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -1.2522278 1.1707296\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 95.0ms\n",
      "Speed: 2.6ms preprocess, 95.0ms inference, 42.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  1.0059662 0.25448275\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 96.0ms\n",
      "Speed: 2.2ms preprocess, 96.0ms inference, 43.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  1.7221222 1.6128993\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 99.2ms\n",
      "Speed: 3.1ms preprocess, 99.2ms inference, 47.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  0.025878906 0.8649602\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 118.8ms\n",
      "Speed: 2.5ms preprocess, 118.8ms inference, 54.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  -0.2844696 0.7382493\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 120.2ms\n",
      "Speed: 2.1ms preprocess, 120.2ms inference, 56.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  32.148804 8.701952\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.8ms\n",
      "Speed: 2.2ms preprocess, 132.8ms inference, 61.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  206.11719 20.984161\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.5ms\n",
      "Speed: 2.5ms preprocess, 133.5ms inference, 71.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "Head change and angle change:  237.53487 11.773098\n",
      "falling\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.5ms\n",
      "Speed: 2.6ms preprocess, 132.5ms inference, 68.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.3ms\n",
      "Speed: 2.4ms preprocess, 134.3ms inference, 101.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.6ms\n",
      "Speed: 2.3ms preprocess, 134.6ms inference, 76.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.3ms\n",
      "Speed: 2.4ms preprocess, 134.3ms inference, 77.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.6ms\n",
      "Speed: 2.4ms preprocess, 134.6ms inference, 75.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 135.1ms\n",
      "Speed: 2.5ms preprocess, 135.1ms inference, 74.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 136.2ms\n",
      "Speed: 2.4ms preprocess, 136.2ms inference, 74.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.4ms\n",
      "Speed: 3.3ms preprocess, 134.4ms inference, 99.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.9ms\n",
      "Speed: 2.0ms preprocess, 133.9ms inference, 77.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.2ms\n",
      "Speed: 2.9ms preprocess, 134.2ms inference, 70.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.1ms\n",
      "Speed: 3.0ms preprocess, 134.1ms inference, 76.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.9ms\n",
      "Speed: 2.7ms preprocess, 133.9ms inference, 77.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 140.7ms\n",
      "Speed: 2.5ms preprocess, 140.7ms inference, 76.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 139.7ms\n",
      "Speed: 3.1ms preprocess, 139.7ms inference, 78.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 135.2ms\n",
      "Speed: 2.2ms preprocess, 135.2ms inference, 75.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 137.2ms\n",
      "Speed: 3.1ms preprocess, 137.2ms inference, 77.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.6ms\n",
      "Speed: 2.1ms preprocess, 132.6ms inference, 72.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 136.5ms\n",
      "Speed: 2.3ms preprocess, 136.5ms inference, 76.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 137.9ms\n",
      "Speed: 2.2ms preprocess, 137.9ms inference, 76.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.5ms\n",
      "Speed: 2.5ms preprocess, 133.5ms inference, 77.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 130.9ms\n",
      "Speed: 2.4ms preprocess, 130.9ms inference, 74.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 136.0ms\n",
      "Speed: 1.9ms preprocess, 136.0ms inference, 71.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 130.8ms\n",
      "Speed: 2.1ms preprocess, 130.8ms inference, 69.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.9ms\n",
      "Speed: 2.2ms preprocess, 132.9ms inference, 75.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.5ms\n",
      "Speed: 2.6ms preprocess, 133.5ms inference, 76.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.8ms\n",
      "Speed: 3.0ms preprocess, 132.8ms inference, 76.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 129.7ms\n",
      "Speed: 2.8ms preprocess, 129.7ms inference, 77.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.3ms\n",
      "Speed: 2.2ms preprocess, 133.3ms inference, 74.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.7ms\n",
      "Speed: 2.7ms preprocess, 132.7ms inference, 74.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 139.0ms\n",
      "Speed: 2.6ms preprocess, 139.0ms inference, 76.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 128.8ms\n",
      "Speed: 2.5ms preprocess, 128.8ms inference, 70.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 134.1ms\n",
      "Speed: 2.3ms preprocess, 134.1ms inference, 76.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.7ms\n",
      "Speed: 2.1ms preprocess, 133.7ms inference, 78.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.5ms\n",
      "Speed: 2.0ms preprocess, 133.5ms inference, 76.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.0ms\n",
      "Speed: 2.3ms preprocess, 133.0ms inference, 75.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.1ms\n",
      "Speed: 2.3ms preprocess, 132.1ms inference, 75.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.5ms\n",
      "Speed: 2.7ms preprocess, 133.5ms inference, 75.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.2ms\n",
      "Speed: 2.1ms preprocess, 133.2ms inference, 68.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 135.0ms\n",
      "Speed: 2.4ms preprocess, 135.0ms inference, 78.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 131.6ms\n",
      "Speed: 2.4ms preprocess, 131.6ms inference, 74.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.9ms\n",
      "Speed: 2.6ms preprocess, 133.9ms inference, 76.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.6ms\n",
      "Speed: 2.3ms preprocess, 133.6ms inference, 72.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 132.5ms\n",
      "Speed: 2.4ms preprocess, 132.5ms inference, 79.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 135.2ms\n",
      "Speed: 2.0ms preprocess, 135.2ms inference, 70.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 136.8ms\n",
      "Speed: 2.6ms preprocess, 136.8ms inference, 80.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 135.4ms\n",
      "Speed: 2.4ms preprocess, 135.4ms inference, 70.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 133.4ms\n",
      "Speed: 2.2ms preprocess, 133.4ms inference, 78.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 120.1ms\n",
      "Speed: 2.2ms preprocess, 120.1ms inference, 62.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 119.8ms\n",
      "Speed: 2.0ms preprocess, 119.8ms inference, 62.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 121.4ms\n",
      "Speed: 2.5ms preprocess, 121.4ms inference, 58.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n",
      "0: 384x640 2 persons, 125.0ms\n",
      "Speed: 2.0ms preprocess, 125.0ms inference, 57.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.0\n",
      "Frame ID: 0.0\n",
      "here\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output_dir, people = \u001b[43mrun_fall_detection_live\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest0.mp4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_fall_detection_live\u001b[39m\u001b[34m(input_video_path, output_filename, min_area)\u001b[39m\n\u001b[32m     53\u001b[39m frame_height, frame_width = frame.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# results = model(frame)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./configs/bytetrack.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.35\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n\u001b[32m     58\u001b[39m timestamp_sec = timestamp_ms / \u001b[32m1000.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:579\u001b[39m, in \u001b[36mModel.track\u001b[39m\u001b[34m(self, source, stream, persist, **kwargs)\u001b[39m\n\u001b[32m    577\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[32m    578\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:536\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:225\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:40\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         response = gen.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     44\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:717\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:142\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:159\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:181\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    182\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/modules/upsampling.py:174\u001b[39m, in \u001b[36mUpsample.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Fall_Detection/.venv/lib/python3.11/site-packages/torch/nn/functional.py:4812\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[32m   4810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4811\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-argument-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4814\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-argument-type]\u001b[39;00m\n\u001b[32m   4815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_dir, people = run_fall_detection_live(0, output_filename='test0.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "96b09b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fall_id': 0,\n",
       "  'person_id': '51',\n",
       "  'angle_changes': [{'timestamp': 2.8666666666666667,\n",
       "    'frame_id': 87.0,\n",
       "    'angle': 81.18475341796875,\n",
       "    'angle_change': 10.3150634765625,\n",
       "    'head_y': 218.94483947753906,\n",
       "    'shoulder_y': 228.23509216308594,\n",
       "    'head_change': -7.8128509521484375}],\n",
       "  'fall_frame': 87.0,\n",
       "  'head_speed': 0.0,\n",
       "  'shoulder_speed': 0.0,\n",
       "  'fall_start': (87.0, 2.8666666666666667),\n",
       "  'fall_end': (87.0, 2.8666666666666667)}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[51]['falls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55e8e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': 2.8, 'frame_id': 85.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 78.80101776123047, 'angle_change': 2.0325469970703125, 'head_y': 224.60008239746094, 'shoulder_y': 236.91551208496094, 'head_change': 4.770416259765625, 'fall_ongoing': False}\n",
      "{'timestamp': 2.8333333333333335, 'frame_id': 86.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 70.86968994140625, 'angle_change': 7.931327819824219, 'head_y': 226.7576904296875, 'shoulder_y': 239.67813110351562, 'head_change': 2.1576080322265625, 'fall_ongoing': False}\n",
      "{'timestamp': 2.8666666666666667, 'frame_id': 87.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 81.18475341796875, 'angle_change': 10.3150634765625, 'head_y': 218.94483947753906, 'shoulder_y': 228.23509216308594, 'head_change': -7.8128509521484375, 'fall_ongoing': True}\n",
      "{'timestamp': 2.9, 'frame_id': 88.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 69.84111785888672, 'angle_change': 11.343635559082031, 'head_y': 218.1195526123047, 'shoulder_y': 231.5404052734375, 'head_change': -0.825286865234375, 'fall_ongoing': True}\n",
      "{'timestamp': 3.0666666666666664, 'frame_id': 93.0, 'horizontal': np.False_, 'vertical': np.False_, 'angle': 18.636001586914062, 'angle_change': 51.205116271972656, 'head_y': 216.7063446044922, 'shoulder_y': 229.05943298339844, 'head_change': -1.4132080078125, 'fall_ongoing': True}\n",
      "{'timestamp': 3.1, 'frame_id': 94.0, 'horizontal': np.False_, 'vertical': np.False_, 'angle': 30.42206382751465, 'angle_change': 11.786062240600586, 'head_y': 215.4864501953125, 'shoulder_y': 228.8019256591797, 'head_change': -1.2198944091796875, 'fall_ongoing': True}\n",
      "{'timestamp': 3.1666666666666665, 'frame_id': 96.0, 'horizontal': np.False_, 'vertical': False, 'angle': 4.677271842956543, 'angle_change': 25.744792938232422, 'head_y': 214.87664794921875, 'shoulder_y': 224.85174560546875, 'head_change': -0.60980224609375, 'fall_ongoing': True}\n",
      "{'timestamp': 3.2333333333333334, 'frame_id': 98.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 64.57796478271484, 'angle_change': 59.900691986083984, 'head_y': 210.529541015625, 'shoulder_y': 225.0625, 'head_change': -4.34710693359375, 'fall_ongoing': True}\n",
      "{'timestamp': 3.3, 'frame_id': 100.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 84.9088134765625, 'angle_change': 20.330848693847656, 'head_y': 208.76893615722656, 'shoulder_y': 222.36181640625, 'head_change': -1.7606048583984375, 'fall_ongoing': True}\n",
      "{'timestamp': 3.3333333333333335, 'frame_id': 101.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 84.4781494140625, 'angle_change': 0.4306640625, 'head_y': 208.9127197265625, 'shoulder_y': 222.7729034423828, 'head_change': 0.1437835693359375, 'fall_ongoing': True}\n",
      "{'timestamp': 3.3666666666666667, 'frame_id': 102.0, 'horizontal': np.True_, 'vertical': np.False_, 'angle': 59.6231689453125, 'angle_change': 24.85498046875, 'head_y': 208.8096160888672, 'shoulder_y': 221.65245056152344, 'head_change': -0.1031036376953125, 'fall_ongoing': True}\n"
     ]
    }
   ],
   "source": [
    "for frame in people[51]['frames'][80:]:\n",
    "    print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97317b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
